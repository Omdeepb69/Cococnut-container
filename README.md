# ü•• Project Coconut: The S-Tier AI MLOps Harness (v1.1.0)

**Project Coconut** is a professional, modular AI ecosystem designed for high-performance production deployments. It bridges the gap between raw models and scalable services with its unique 5-layer architecture.

---

## üèõÔ∏è S-Tier Architecture (The "Modular Armor")

1.  **Hardware Auto-Sensing**: Automatically detects NVIDIA GPUs and applies FP16 precision. Falls back to optimized FP32 on CPUs.
2.  **Semantic Inference Cache**: Verified **99% latency reduction** for repeat queries. Responds in <1s by reusing previous model logic.
3.  **Dynamic Feature Toggles**: Enable/Disable RAG, Semantic Caching, and Session Memory instantly via environment variables.
4. **Step 4: CI/CD Automation**
   Automate your releases with **GitHub Actions**. See our [CI/CD Pipeline Tutorial](file:///home/omdeep-borkar/.gemini/antigravity/brain/9f6d224a-9043-46d6-a450-a3e2bc1abf41/ci_cd_tutorial.md) for details on:
   - Automated Docker Builds.
   - **Trivy** Security Scanning (Fail-on-Vulnerability).
   - Automated production deployment to Docker Hub.

---

## üöÄ The S-Tier Setup Workflow (Step-by-Step)

Follow this exact sequence to go from a clean server to a production-ready AI backend.

### Step 1: Initialize the Stack
```bash
# Clone the repo
git clone https://github.com/Omdeepb69/Cococnut-container.git
cd coconut

# Build and start in detached mode
docker compose up --build -d
```

### Step 2: Establish Identity (Generate API Key)
The API is locked by default. Generate your first production key:
```bash
# This creates a 'pro' tier key with 100 req/min limit
curl -X POST "http://localhost:8000/generate-key?tier=pro"
```
> [!IMPORTANT]
> Save the returned `api_key` immediately. It is hashed for security and cannot be shown again.

### Step 3: Seed the Knowledge (RAG Ingestion)
Populate your Vector Database with the facts the AI should know:
```bash
docker compose exec coconut-api python3 ingest.py "Project Coconut is a scalable AI harness by Omdeep."
```

### Step 4: Verify the Pipeline
```bash
# Replace YOUR_KEY with the key from Step 2
curl -X POST "http://localhost:8000/chat" \
     -H "X-API-Key: YOUR_KEY" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "What is Project Coconut?", "session_id": "init_test"}'
```

---

## üõ†Ô∏è How to "Edit Setup" (Configuration)

Project Coconut is designed to be modified without changing code. All configuration happens in the `docker-compose.yml` file.

### 1. Changing the AI Model
To swap the "Brain", edit the `MODEL_ID` in `docker-compose.yml`:
```yaml
environment:
  - MODEL_ID=gpt2  # Change this to any HuggingFace Repo ID
```
Then run: `docker compose up -d` to reload.

### 2. Toggling Production Features
Turn features on/off instantly via these flags in `docker-compose.yml`:
- `ENABLE_RAG`: Set `False` to disable knowledge-grounding.
- `ENABLE_CACHE`: Set `False` to disable semantic response reuse.
- `ENABLE_MEMORY`: Set `False` to disable session history.

### 3. Hardware Optimization
By default, the system uses `DEVICE=auto`. To force a specific mode:
- For **GPU**: Set `DEVICE=cuda`.
- For **CPU**: Set `DEVICE=cpu`.

---

## üíª Technical Command Reference

| Area | Command | Purpose |
| :--- | :--- | :--- |
| **Lifecycle** | `docker compose up -d` | Start/Update the stack. |
| **Lifecycle** | `docker compose down` | Fully stop the system. |
| **Logs** | `docker compose logs -f` | Watch live hardware detection. |
| **Knowledge** | `python3 ingest.py "text"` | Add data to RAG database. |
| **Metrics** | `curl localhost:8000/metrics` | View cache hit rates & count. |
| **Security** | `curl localhost:8000/health` | Check API & Redis health. |

---

## üìà Performance & Scaling

- **Cold Start (CPU)**: ~160s
- **Hot Cache (Reuse)**: **< 1s**

For 1 Million Users, see the [Scaling Roadmap](file:///home/omdeep-borkar/.gemini/antigravity/brain/9f6d224a-9043-46d6-a450-a3e2bc1abf41/scaling_guide.md).

---
*Created with ‚ù§Ô∏è by the Project Coconut Team.*
